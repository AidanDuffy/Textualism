{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview / Background for Math Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our team's goal with this project is to develop a process for measuring and visualizing the evolving sense of controversial constitutional terminology. The bulk of this work hinges on Word2Vec, an NLP API that allows for the embedding of words as vectors. In vectorizing words, we open up avenues for the analysis of linguistic relationships through geometric means. \n",
    "\n",
    "In essence the idea is, given a set of historical documents, we want to generate the \"optimal\" embedding (which we will discuss more later), and then try to extract a geometric context for controversial words, such as \"arms\". \n",
    "\n",
    "Word embeddings need to capture an incredible amount of complexity and variation, so the word vectors we produce have dimension on the order of tens. In these high-dimensional spaces, the Euclidean norm starts to lose it's ability to \"fairly\" represent distance, so we instead use the cosine-norm for all distance calculations.\n",
    "\n",
    "$$ cosine\\_norm(u,v) = \\frac{u \\cdot v}{|u|~|v|} $$ \n",
    "\n",
    "Another side-effect of high-dimensions is the obvious difficulty to produce an intelligible visualization. For this reason, in the Network Creation section of our project, we implemented Singular Value Decomposition. SVD is a process for decomposing a rank-r matrix into a sum of r rank-1 matrices. \n",
    "\n",
    "Suppose our word embedding process has generated a set of 2000 word vectors of dimension 40 (these are realistic values for this project). These vectors \"live\" in the standard basis of $R^{40}$. Let's see if we can simplify that basis into something more amenable to visualization while losing as little information as possible. We place all of our vectors into a matrix A as row vectors.\n",
    "$$ A = \\sigma_1 \\vec{u_1} \\vec{v_1}^T + \\sigma_2 \\vec{u_2} \\vec{v_2}^T ... + \\sigma_r \\vec{u_r} \\vec{v_r}^T$$\n",
    "It is relevant here because, much like diagonalization, it allows one to obtain a potentially more useful basis for a set of vectors. For our networks, we populate $A$ with our word vectors as rows, and then attain an approximation for A by summing our  three $\\sigma_i \\vec{u_i} \\vec{v_i}^T$ terms with highest corresponding singular values ($\\sigma_1,\\sigma_2,\\sigma_3$). Another way of saying this is (supposing we order our singular values in descending order):\n",
    "\n",
    "$$A \\approx \\sigma_1 \\vec{u_1} \\vec{v_1}^T + \\sigma_2 \\vec{u_2} \\vec{v_2}^T + \\sigma_3 \\vec{u_3} \\vec{v_3}^T$$\n",
    "\n",
    "This approximation allows us to create a new, 3-dimensional basis for our word vectors:\n",
    "\n",
    "$$B_{reduced} = \\{ \\vec{v_1},\\vec{v_2},\\vec{v_3}\\} $$\n",
    "\n",
    "The basis vectors for $B_{reduced}$ still live in $R^{40}$. However, by projecting each of our word vectors onto this basis, we can achieve an approximation for our word vectors in $R^3$. This lets us visualize the structure of a 40-dimensional space with a graph in 3 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  * For each era:\n",
    "    * Parse the relevant set of documents\n",
    "    * For each \"reasonable\" set of constructor parameters\n",
    "      * Initialize a Word2Vec model with parameters\n",
    "      * Calculate WordSim353 similarity Spearmen correlation for model\n",
    "    * Choose the model which maximizes WordSim353 Spearmen\n",
    "    * Analyze the \"neighborhood\" of chosen words through:\n",
    "      * Creation of a graph based on Word2Vec's word similarity metric\n",
    "      %* K-Means Clustering of embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing\n",
    "\n",
    "In this step, we take in a set of text documents and tokenize it by sentences and then by words. The result is a list of lists of words, which we will pass to the Word2Vec constructor shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Duffy\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'web'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2e048b73930e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0meval_embedding\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0meval_sim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mSENTENCE_TOKENIZER\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./nltk_data/tokenizers/punkt/english.pickle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mQUOTES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\u201c|\\u201d\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Duffy\\Documents\\Textualism\\Textualism\\eval_embedding.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mweb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mevaluate_similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mword_embeddings_benchmarks_master\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilarity\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfetch_WS353\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mClustering\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'web'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import sys, re, os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from eval_embedding import eval_sim\n",
    "SENTENCE_TOKENIZER = nltk.data.load('./nltk_data/tokenizers/punkt/english.pickle') \n",
    "QUOTES = re.compile(\"\\u201c|\\u201d\")\n",
    "\n",
    "def read_dir(path):\n",
    "    \"\"\"\n",
    "    @param path: (type = str) path to dir that containes files\n",
    "    @return: (type=str)\n",
    "    \"\"\"\n",
    "    assert os.path.exists(path)\n",
    "    file_names = os.listdir(path)\n",
    "    return read(file_names, path)\n",
    "\n",
    "def read(file_names, path):\n",
    "    \"\"\"\n",
    "    Reads in a text file as a string, returning the stringified version\n",
    "    @param file_names: (type=list<str>) files to be read\n",
    "    @param path: (type = str) path to dir that containes files\n",
    "    @return: (type=str)\n",
    "    \"\"\"\n",
    "    data = \"\"\n",
    "    for file_name in file_names:\n",
    "        with open(path + \"/\" + file_name, \"r\") as f:\n",
    "            data += re.sub(QUOTES, \"\\\"\", f.read())\n",
    "            data += \"\\n\"\n",
    "    return data\n",
    "\n",
    "def parse(text):\n",
    "    \"\"\"\n",
    "    Parses a stringified file into sentences, a list of lists of words\n",
    "    @param text: (type=str) text to parse\n",
    "    @return: (type=list<list<str>>) parsed text \n",
    "    \"\"\"\n",
    "    sentences = SENTENCE_TOKENIZER.tokenize(text)\n",
    "    sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "data = read_dir(\"Modern_Era/2A\")\n",
    "sentences = parse(data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding/Optimization\n",
    "\n",
    "We now take the parsed text from the previous step, and pass it to our embedding initializer. We chose 3 model parameters to optimize over:\n",
    "* size: the dimension of the produced word vectors\n",
    "* window: the 'radius' around a word that Word2Vec will inspect\n",
    "* min_count: the minimum frequency for a word to be included in the model\n",
    "\n",
    "We chose acceptable ranges for each of those parameters and then evaluated every Word2Vec embedding for this particular text within those parameter ranges. We use WordSim353's similarity metric, which measures the correlation of cosine similarity of word vectors with human reported similarity of words. The model that we output is the model with the highest WordSim353 similarity correlation (as similarity is central to our processing of the embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b17a6c87bd1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;31m#model.save(\"second_amendment_modern_demo.bin\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"second_amendment.bin\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "def init_model(sentences):\n",
    "    \"\"\"\n",
    "    Initializes optimal Word2Vec model for text from sentences\n",
    "    @param sentences: (type=list<list<str>>) a list of lists of words\n",
    "    @return: (type=Word2Vec) model\n",
    "    \"\"\"\n",
    "    best_spearmen = 0\n",
    "    best_model = None\n",
    "    best_params = (0,0,0)\n",
    "    count = 0\n",
    "    for dim in range(20, 61):\n",
    "        for window in range(5, 11):\n",
    "            for min_count in range(2,5):\n",
    "                model = Word2Vec(sentences, min_count=min_count, window=window, size=dim)\n",
    "                similarity_spearmen = eval_sim(model)\n",
    "                if similarity_spearmen > best_spearmen:\n",
    "                    best_model = model\n",
    "                    best_params = (min_count, window, dim)\n",
    "                    best_spearmen = similarity_spearmen\n",
    "                #print(str(count) + \": \" + str(similarity_spearmen))\n",
    "                count += 1\n",
    "    print(\"min_count: \" + str(best_params[0]), \n",
    "        \"window: \" + str(best_params[1]), \n",
    "        \"dim: \" + str(best_params[2]))\n",
    "    print(\"correlation: \" + str(best_spearmen))\n",
    "    return best_model\n",
    "\n",
    "model = init_model(sentences)\n",
    "#model.save(\"second_amendment_modern_demo.bin\")\n",
    "model.save(\"second_amendment.bin\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighborhood Creation/Dimensionality Reduction\n",
    "\n",
    "We now use Word2Vec's built in similarity measure to extract the 10 most similar words to our \"main_word\". This set of 11 words constitutes a \"neighborhood\". We perform SVD on this neighborhood of word vectors and pass the reduced dimension word vectors along to the visualization stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Neighborhood:\n",
    "    def __init__(self, word, model, neighbors=10):\n",
    "        \"\"\"\n",
    "        @param word: (type=str) \n",
    "        @param model: (type=Word2Vec Model) <(neighboring_word, edge_weight)>\n",
    "        @field word: (type=str)\n",
    "        @field similarity_neighbors: (type=list<tuple<str, float>>) neighbors to word based on \n",
    "        @field proximity_neighbors: (type=list<tuple<str, float>>) neighbors to word based on cosine_distance \n",
    "        \"\"\"\n",
    "        self.word = word\n",
    "        self.similarity_neighbors = model.wv.most_similar(positive=[word], topn=neighbors)\n",
    "\n",
    "def get_neighboring_words(word, model, n=10, verbose=False):\n",
    "    n = Neighborhood(word, model, neighbors=n)\n",
    "    if verbose:\n",
    "        print(n.word)\n",
    "        print(\"Similarity Neighbors\")\n",
    "    words = []\n",
    "    for neighbor in n.similarity_neighbors:\n",
    "        if verbose:\n",
    "            print(neighbor)\n",
    "        words.append(neighbor[0])\n",
    "    return words, n.similarity_neighbors\n",
    "\n",
    "def get_svd_from_words(model, words, verbose=False):\n",
    "    vecs = [model.wv[word] for word in words]\n",
    "    mat = np.stack(vecs, axis=0)\n",
    "    if verbose:\n",
    "        print('shape of word embeddngs matrix:', mat.shape) #shape is (neighbors,32), neighbors defaults to 10\n",
    "    U, s, V = np.linalg.svd(mat)\n",
    "    if verbose:\n",
    "        print('singular values:', s)   # Take a quick look at svd_test.py (run it) if you want to convince yourself of how svd works for m by n matrices\n",
    "               # We basically want the first three row vectors in V; these are the eigenvectors that explain most of the variation in the rows (i.e. word embeddings) of the original matrix.\n",
    "    return U, s, V\n",
    "\n",
    "def get_coords_from_svd_projection(V, model, words, verbose=False):\n",
    "    V_cut = V[:3,:]\n",
    "    if verbose:\n",
    "        print('matrix after removing less import eigenvectors:')\n",
    "        print(V_cut)\n",
    "        print('squared row magnitudes:')\n",
    "        for i in range(3):     # this yields 1.0 every time, i.e. to compute projection coordinates we can ignore the a.a on the denominator (V_cut has unitary rows)\n",
    "            print(V_cut[i,:].dot(V_cut[i,:]))\n",
    "\n",
    "\n",
    "    #for each word, associate it with its projection coordinates in the V_cut basis.\n",
    "    # This associates each words with a ordered triple of points, which allows us to graph in 3d,\n",
    "    # or 2d (you can just use the first two coordinates if you want). One idea for the future would\n",
    "    #be to label the axes of the graph with the word that the corresponding basis vector of our graph is closest to.\n",
    "    return V_cut, {w: V_cut.dot(model.wv[w]) for w in words}\n",
    "\n",
    "\"\"\"\n",
    "Main utility method for any users of this file. Bundles up three sub-processes to allow you \n",
    "to get a three dimensional space on which to plot the similar words to the given one based on some model\n",
    "\n",
    "The current method computes the svd of the similar words, along with the original word, but throws out words of length\n",
    "2 or smaller by default.\n",
    "\"\"\"\n",
    "def get_points_from_word_and_model(word, model_path, verbose=False, bigger_than=2):\n",
    "    model = Word2Vec.load(model_path)\n",
    "    \n",
    "    #gets the n \"most similar\" words to the initial word in this model. \n",
    "    #Also returns a dict containing those similarity scores, which is not used in this method\n",
    "    words,_ = get_neighboring_words(word,model, n=10, verbose=verbose)\n",
    "\n",
    "    # an intermediate processing step here that removes small words and adds in the main word we're considering?\n",
    "    cond = (lambda w: len(w) > bigger_than) if bigger_than > 0 else (lambda w: True)\n",
    "    words = [word] + [w for w in words if cond(w)]\n",
    "\n",
    "    # computes the svd of the matrix containing the embeddings of the 10 most similar words as rows. \n",
    "    # U and s are not used, but are left in for readability. s contains the singular values, set verbose=True to print them.\n",
    "    U, s, V = get_svd_from_words(model, words, verbose=verbose)\n",
    "    \n",
    "    basis, coords = get_coords_from_svd_projection(V, model, words, verbose=verbose)\n",
    "\n",
    "    return basis, coords, model\n",
    "\n",
    "word = \"arms\"\n",
    "model_path = \"second_amendment.bin\"\n",
    "basis, coords, _ = get_points_from_word_and_model(word, model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighborhood Visualization\n",
    "In this stage, we plot the neighborhood in our new 3-dimensional basis and create a labeled visualization of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "<gensim.models.keyedvectors.KeyedVectors object at 0x1191199e8>\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "hoverinfo": "none",
         "line": {
          "color": "rgb(125,125,125)",
          "width": 1
         },
         "mode": "lines",
         "type": "scatter3d",
         "x": [
          -7.98871374130249,
          -0.6621240377426147,
          null,
          -7.98871374130249,
          -7.180265426635742,
          null,
          -7.98871374130249,
          -0.49025169014930725,
          null,
          -7.98871374130249,
          -0.7769057154655457,
          null,
          -7.98871374130249,
          -3.7537665367126465,
          null,
          -7.98871374130249,
          -7.811208724975586,
          null,
          -7.98871374130249,
          -0.8228949904441833,
          null,
          -7.98871374130249,
          -0.9574193954467773,
          null,
          -7.98871374130249,
          -1.4337464570999146,
          null
         ],
         "y": [
          -0.303531676530838,
          -0.12208102643489838,
          null,
          -0.303531676530838,
          1.2253767251968384,
          null,
          -0.303531676530838,
          -0.09056109189987183,
          null,
          -0.303531676530838,
          -0.06272789090871811,
          null,
          -0.303531676530838,
          -0.6841533184051514,
          null,
          -0.303531676530838,
          -0.37965020537376404,
          null,
          -0.303531676530838,
          -0.0595761314034462,
          null,
          -0.303531676530838,
          -0.1918996423482895,
          null,
          -0.303531676530838,
          -0.30222007632255554,
          null
         ],
         "z": [
          -0.21398845314979553,
          0.04800281301140785,
          null,
          -0.21398845314979553,
          0.0360022634267807,
          null,
          -0.21398845314979553,
          0.045671023428440094,
          null,
          -0.21398845314979553,
          -0.007515208795666695,
          null,
          -0.21398845314979553,
          0.05288771539926529,
          null,
          -0.21398845314979553,
          0.14279039204120636,
          null,
          -0.21398845314979553,
          0.08318634331226349,
          null,
          -0.21398845314979553,
          0.008651027455925941,
          null,
          -0.21398845314979553,
          0.008384715765714645,
          null
         ]
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": "rgb(175,175,175)",
          "line": {
           "color": "rgb(50,50,50)",
           "width": 0.5
          },
          "size": 6,
          "symbol": "dot"
         },
         "mode": "markers",
         "text": [
          "guaranteed",
          "bear",
          "enforceable",
          "arms",
          "infringed",
          "people",
          "right",
          "applies",
          "privilege",
          "carry"
         ],
         "type": "scatter3d",
         "x": [
          -0.6621240377426147,
          -7.180265426635742,
          -0.49025169014930725,
          -7.98871374130249,
          -0.7769057154655457,
          -3.7537665367126465,
          -7.811208724975586,
          -0.8228949904441833,
          -0.9574193954467773,
          -1.4337464570999146
         ],
         "y": [
          -0.12208102643489838,
          1.2253767251968384,
          -0.09056109189987183,
          -0.303531676530838,
          -0.06272789090871811,
          -0.6841533184051514,
          -0.37965020537376404,
          -0.0595761314034462,
          -0.1918996423482895,
          -0.30222007632255554
         ],
         "z": [
          0.04800281301140785,
          0.0360022634267807,
          0.045671023428440094,
          -0.21398845314979553,
          -0.007515208795666695,
          0.05288771539926529,
          0.14279039204120636,
          0.08318634331226349,
          0.008651027455925941,
          0.008384715765714645
         ]
        }
       ],
       "layout": {
        "height": 1000,
        "hovermode": "closest",
        "margin": {
         "t": 100
        },
        "scene": {
         "xaxis": {
          "showbackground": false,
          "showgrid": false,
          "showline": false,
          "showticklabels": false,
          "title": " ",
          "zeroline": false
         },
         "yaxis": {
          "showbackground": false,
          "showgrid": false,
          "showline": false,
          "showticklabels": false,
          "title": " ",
          "zeroline": false
         },
         "zaxis": {
          "showbackground": false,
          "showgrid": false,
          "showline": false,
          "showticklabels": false,
          "title": " ",
          "zeroline": false
         }
        },
        "showlegend": false,
        "title": "word relationships with respect to arms",
        "width": 1000
       }
      },
      "text/html": [
       "<div id=\"e8eafc8e-0446-4fd5-b7c0-d9dd527c4a0b\" style=\"height: 1000px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"e8eafc8e-0446-4fd5-b7c0-d9dd527c4a0b\", [{\"mode\": \"lines\", \"z\": [-0.21398845314979553, 0.04800281301140785, null, -0.21398845314979553, 0.0360022634267807, null, -0.21398845314979553, 0.045671023428440094, null, -0.21398845314979553, -0.007515208795666695, null, -0.21398845314979553, 0.05288771539926529, null, -0.21398845314979553, 0.14279039204120636, null, -0.21398845314979553, 0.08318634331226349, null, -0.21398845314979553, 0.008651027455925941, null, -0.21398845314979553, 0.008384715765714645, null], \"hoverinfo\": \"none\", \"type\": \"scatter3d\", \"y\": [-0.303531676530838, -0.12208102643489838, null, -0.303531676530838, 1.2253767251968384, null, -0.303531676530838, -0.09056109189987183, null, -0.303531676530838, -0.06272789090871811, null, -0.303531676530838, -0.6841533184051514, null, -0.303531676530838, -0.37965020537376404, null, -0.303531676530838, -0.0595761314034462, null, -0.303531676530838, -0.1918996423482895, null, -0.303531676530838, -0.30222007632255554, null], \"x\": [-7.98871374130249, -0.6621240377426147, null, -7.98871374130249, -7.180265426635742, null, -7.98871374130249, -0.49025169014930725, null, -7.98871374130249, -0.7769057154655457, null, -7.98871374130249, -3.7537665367126465, null, -7.98871374130249, -7.811208724975586, null, -7.98871374130249, -0.8228949904441833, null, -7.98871374130249, -0.9574193954467773, null, -7.98871374130249, -1.4337464570999146, null], \"line\": {\"color\": \"rgb(125,125,125)\", \"width\": 1}}, {\"mode\": \"markers\", \"y\": [-0.12208102643489838, 1.2253767251968384, -0.09056109189987183, -0.303531676530838, -0.06272789090871811, -0.6841533184051514, -0.37965020537376404, -0.0595761314034462, -0.1918996423482895, -0.30222007632255554], \"text\": [\"guaranteed\", \"bear\", \"enforceable\", \"arms\", \"infringed\", \"people\", \"right\", \"applies\", \"privilege\", \"carry\"], \"hoverinfo\": \"text\", \"type\": \"scatter3d\", \"z\": [0.04800281301140785, 0.0360022634267807, 0.045671023428440094, -0.21398845314979553, -0.007515208795666695, 0.05288771539926529, 0.14279039204120636, 0.08318634331226349, 0.008651027455925941, 0.008384715765714645], \"x\": [-0.6621240377426147, -7.180265426635742, -0.49025169014930725, -7.98871374130249, -0.7769057154655457, -3.7537665367126465, -7.811208724975586, -0.8228949904441833, -0.9574193954467773, -1.4337464570999146], \"marker\": {\"color\": \"rgb(175,175,175)\", \"symbol\": \"dot\", \"size\": 6, \"line\": {\"color\": \"rgb(50,50,50)\", \"width\": 0.5}}}], {\"margin\": {\"t\": 100}, \"title\": \"word relationships with respect to arms\", \"hovermode\": \"closest\", \"showlegend\": false, \"scene\": {\"zaxis\": {\"showgrid\": false, \"title\": \" \", \"zeroline\": false, \"showbackground\": false, \"showticklabels\": false, \"showline\": false}, \"yaxis\": {\"showgrid\": false, \"title\": \" \", \"zeroline\": false, \"showbackground\": false, \"showticklabels\": false, \"showline\": false}, \"xaxis\": {\"showgrid\": false, \"title\": \" \", \"zeroline\": false, \"showbackground\": false, \"showticklabels\": false, \"showline\": false}}, \"height\": 1000, \"width\": 1000}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"e8eafc8e-0446-4fd5-b7c0-d9dd527c4a0b\" style=\"height: 1000px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"e8eafc8e-0446-4fd5-b7c0-d9dd527c4a0b\", [{\"mode\": \"lines\", \"z\": [-0.21398845314979553, 0.04800281301140785, null, -0.21398845314979553, 0.0360022634267807, null, -0.21398845314979553, 0.045671023428440094, null, -0.21398845314979553, -0.007515208795666695, null, -0.21398845314979553, 0.05288771539926529, null, -0.21398845314979553, 0.14279039204120636, null, -0.21398845314979553, 0.08318634331226349, null, -0.21398845314979553, 0.008651027455925941, null, -0.21398845314979553, 0.008384715765714645, null], \"hoverinfo\": \"none\", \"type\": \"scatter3d\", \"y\": [-0.303531676530838, -0.12208102643489838, null, -0.303531676530838, 1.2253767251968384, null, -0.303531676530838, -0.09056109189987183, null, -0.303531676530838, -0.06272789090871811, null, -0.303531676530838, -0.6841533184051514, null, -0.303531676530838, -0.37965020537376404, null, -0.303531676530838, -0.0595761314034462, null, -0.303531676530838, -0.1918996423482895, null, -0.303531676530838, -0.30222007632255554, null], \"x\": [-7.98871374130249, -0.6621240377426147, null, -7.98871374130249, -7.180265426635742, null, -7.98871374130249, -0.49025169014930725, null, -7.98871374130249, -0.7769057154655457, null, -7.98871374130249, -3.7537665367126465, null, -7.98871374130249, -7.811208724975586, null, -7.98871374130249, -0.8228949904441833, null, -7.98871374130249, -0.9574193954467773, null, -7.98871374130249, -1.4337464570999146, null], \"line\": {\"color\": \"rgb(125,125,125)\", \"width\": 1}}, {\"mode\": \"markers\", \"y\": [-0.12208102643489838, 1.2253767251968384, -0.09056109189987183, -0.303531676530838, -0.06272789090871811, -0.6841533184051514, -0.37965020537376404, -0.0595761314034462, -0.1918996423482895, -0.30222007632255554], \"text\": [\"guaranteed\", \"bear\", \"enforceable\", \"arms\", \"infringed\", \"people\", \"right\", \"applies\", \"privilege\", \"carry\"], \"hoverinfo\": \"text\", \"type\": \"scatter3d\", \"z\": [0.04800281301140785, 0.0360022634267807, 0.045671023428440094, -0.21398845314979553, -0.007515208795666695, 0.05288771539926529, 0.14279039204120636, 0.08318634331226349, 0.008651027455925941, 0.008384715765714645], \"x\": [-0.6621240377426147, -7.180265426635742, -0.49025169014930725, -7.98871374130249, -0.7769057154655457, -3.7537665367126465, -7.811208724975586, -0.8228949904441833, -0.9574193954467773, -1.4337464570999146], \"marker\": {\"color\": \"rgb(175,175,175)\", \"symbol\": \"dot\", \"size\": 6, \"line\": {\"color\": \"rgb(50,50,50)\", \"width\": 0.5}}}], {\"margin\": {\"t\": 100}, \"title\": \"word relationships with respect to arms\", \"hovermode\": \"closest\", \"showlegend\": false, \"scene\": {\"zaxis\": {\"showgrid\": false, \"title\": \" \", \"zeroline\": false, \"showbackground\": false, \"showticklabels\": false, \"showline\": false}, \"yaxis\": {\"showgrid\": false, \"title\": \" \", \"zeroline\": false, \"showbackground\": false, \"showticklabels\": false, \"showline\": false}, \"xaxis\": {\"showgrid\": false, \"title\": \" \", \"zeroline\": false, \"showbackground\": false, \"showticklabels\": false, \"showline\": false}}, \"height\": 1000, \"width\": 1000}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "from nltk.cluster.util import cosine_distance\n",
    "#working example\n",
    "\"\"\"\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "\n",
    "#when using jupyter notebooks, uncomment the following line and change function call below to plotly.offline.iplot\n",
    "#plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "plotly.offline.plot({\n",
    "    \"data\": [Scatter(x=[1, 2, 3, 4], y=[4, 3, 2, 1])],\n",
    "    \"layout\": Layout(title=\"hello world\")\n",
    "})\n",
    "\"\"\"\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "print(basis.shape[1])\n",
    "print(model.wv)\n",
    "#coords should be a dictionary from a word to a 3d position\n",
    "def generate_network(main_word, coords):\n",
    "\n",
    "    #next step: generate edges.\n",
    "    #   In general, we can modify or optionize this however we want to get whatever edge structures we find interesting. \n",
    "    #   --We might, for example, create more Neighborhoods (one for each \"similar\" word) to see if we get any connections within the 1-level-away words.\n",
    "    #   --We could also try projecting some words from these new neighborhoods (maybe the top three from each?) onto our basis (just dot product) and getting those edges as well.\n",
    "    e_dict = {}\n",
    "    for i in range(3):\n",
    "        e_dict[coord_map[i] + '_e'] = sum([[main_coords[i], coords[w][i], None] for w in id_to_name if w != main_word], [])\n",
    "        e_dict[coord_map[i] + '_n'] = [coords[w][i] for w in id_to_name]\n",
    "\n",
    "    return e_dict\n",
    "\n",
    "def plot_network(g_dict, axis_titles):\n",
    "    trace1=Scatter3d(\n",
    "        x=g_dict['x_e'], y=g_dict['y_e'], z=g_dict['z_e'], mode='lines', line=Line(\n",
    "                    color='rgb(125,125,125)', width=1), hoverinfo='none')\n",
    "\n",
    "    trace2=Scatter3d(x=g_dict['x_n'], y=g_dict['y_n'], z=g_dict['z_n'], mode='markers', marker=Marker(\n",
    "        symbol='dot', size=6, color='rgb(175,175,175)', line = Line(\n",
    "            color='rgb(50,50,50)', width=0.5)), text=id_to_name, hoverinfo='text')\n",
    "\n",
    "    #print(axis_titles)\n",
    "    axes = [dict(showbackground=False, showline=False, zeroline=False, showgrid=False, showticklabels=False, title=' ') for i in range(3)]\n",
    " \n",
    "    title = 'word relationships with respect to ' + main_word\n",
    "    layout=Layout(title=title,width=1000, height=1000, showlegend=False, scene=Scene(\n",
    "        xaxis=XAxis(axes[0]), yaxis=YAxis(axes[1]), zaxis=ZAxis(axes[2])), margin=Margin(t=100), hovermode='closest')\n",
    "    \n",
    "    data = Data([trace1, trace2])\n",
    "    fig=Figure(data=data, layout=layout)\n",
    "    plotly.offline.iplot(fig, filename='arms_network.html')\n",
    "    \n",
    "def get_closest_basis_words(basis, model):\n",
    "    maps = {}\n",
    "    for i in range(basis.shape[0]):\n",
    "        curr_bas = basis[i,:]\n",
    "        closest = ('', 10)\n",
    "        for key in model.wv.vocab:\n",
    "#            if len(key) < 3:\n",
    "#                continue\n",
    "            word = key\n",
    "            vec = model.wv[key]\n",
    "            dist = abs(cosine_distance(vec, curr_bas))\n",
    "            if dist < closest[1]:\n",
    "                closest = (word, dist)\n",
    "        maps[i] = closest\n",
    "    return maps\n",
    "                \n",
    "\n",
    "\n",
    "#basis isn't going to be used yet, but we might label the axes in the future\n",
    "#generate node-id mappings\n",
    "main_word = word\n",
    "name_to_id = {}\n",
    "id_to_name = []\n",
    "temp_id = 0\n",
    "for w in coords:\n",
    "    name_to_id[w] = temp_id\n",
    "    temp_id +=1\n",
    "    id_to_name.append(w)\n",
    "main_coords = coords[main_word] \n",
    "coord_map= ['x','y','z']\n",
    "\n",
    "#generate network\n",
    "g_dict = generate_network(main_word, coords)\n",
    "\n",
    "#plot network\n",
    "plot_network(g_dict, get_closest_basis_words(basis, model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
