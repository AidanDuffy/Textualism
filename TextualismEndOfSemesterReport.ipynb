{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Textualism Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Team:\n",
    "### *Project Manager: Aidan Duffy<br />Computer Science Division: Alex Cegarra (Algorithm Development), Evan Lohn (Algorithm Development & Visualization)<br /> Humanities Division: Tonya Nguyen (Document Research)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The textualism team was tasked with developing a language processing program that determines the meaning of specific and controversial language used in the Constitution. We utilized Python to parse through a number of documents that we gathered from several eras in United States, specifically landmark Supreme Court and several Circuit Court case decisions and opinions. These documents allowed us to determine a more legal and proper defintion of the words. Our program reads these documents, creates and merges word vectors, and then we visualize them in order to show the evolution of these words' meaning.\n",
    "\n",
    "We were unable to finish the project entirely this semester, given our small team, but we have made a substantial amount of progress. We focused primarily on the second amendment, as it seems to be a major topic of discussion and debate. We plan to continute with other parts of the Constitution, such as the fourth amendment, next semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Install Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please download and install the following packages prior to running the rest of the program: <br />\n",
    "*gensim (very important) <br />\n",
    "web (in our repository) <br / > nltk <br / > nltkdata <br / > tqdm <br /> plotly (if you would like to see the visualization for yourself)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1:\n",
    "## Document Research\n",
    "The humanities division of the textualism team laid the initial groundwork for the project. Tonya and Aidan discussed initially how we should approach this topic, since there are so many ways to find documents from all of these eras in our nation's history. At first, we thought we should try to gather as much as possible in any field, whether that be legal documents, such as court decisions or amicus briefs, news articles, or letters (more so targeted for the older time periods). We also thought we could research several parts of the Constitution at one time. However, this not only proved to be incredibly daunting, it also had two major flaws. <br />\n",
    "\n",
    "First, the type of language utilized in these various types of documents would be drastically different. Legal documents would contain a lot of jargon, be incredibly formal, but would target the meaning of the words directly and attempt to interpret them as best as they can. The news articles, whether they be modern and from websites or blogs or older and from newspapers (yes, we know we can still use newspapers! But, nevertheless...), their tone would be significantly less formal and they would use simpler language that the average layperson (or CS student) could understand. On the other hand, they typically have their own personal agenda to put forward on the topic. In our case, since we primarily researched the second amendment, we did not want to cloud our research with people who think we should ban all guns nor with the idea that every child should be handed a firearm after they exit the womb. Finally, since we researched backwards, letters were incredibly difficult to come across, and they also rarely target the specific meaning of words like those used in the Constitution. We may, however, return to these as we research older eras like the years surrounding the writing of the Constitution. Because all of these sources would have totally different types of language, may or may not have an agenda, or may not even be found for the eras we are looking into, we knew that we had to narrow things down quite a bit. <br />\n",
    "\n",
    "In order to narrow things down, we thought that we should specifically target the legal documents, mostly court cases and the judges' opinions. This includes the majority, concurring, and dissenting so we could receive all points of view on the given topic. We believed that this would be the most effective manner of researching because the judges (or justices in the case of the Supreme Court) try to directly interpret the words' meaning with as little bias as possible. After researching, of course, we did discover that there are clear partisan lines even today within the SCOTUS. Several justices, such as Ruth Bader Ginsburg, believe that the Constitution does not and has never guaranteed any US citizen the right to any type of firearm unless they are explicitly in an organized, state regulated militia. While she is certainly not alone on the Court (3 others in 2008), in 2008, the majority of court agreed that the second amendment guarantees the individual's right to own a firearm, regardless of service in any militia. After seeing this, especically because this specific case (District of Columbia v. Heller) had a close 5-4 vote and the justices were on completely opposite ends of the spectrum, we knew that we had to include all of the opinions. There were other cases like this, but I believe that this was a great example of why the courts provide a lot of helpful information about the meaning of these words. <br />\n",
    "\n",
    "Secondly, attempting to research all of the controversial and debated parts of the Constitution was bold, and would have ultimately hurt us had we pursued it. Specifically, it would have hurt our CS division. Not only would the humanities team be divided and a bit disfunctional, grabbing documents related to this amendment and that or this clause and that clause, the CS division would be unable to have a strong set of data to work from to improve upon and refine the algorithms used in this project. After discussing this with Alex, Aidan decided that, in addition to limiting the scope of the research to legal documents only, the group should come together and decide which part of the Constitution we should focus on. In the end, we voted for the second amendment as it is one that is highly controversial, does not have a lot of text within it (it is only about a line instead of the first or fourth amendments' several lines or the fourteenth's multiplte sections. We knew that this would not only be an interesting topic to research, but it would make Alex's, and ultimately Evan's when he joined our team, job a lot easier. <br />\n",
    "\n",
    "From here, our research began in full force. The humanities division focused on the modern era first. Since we were not quite sure as to how to describe what a given era was, we decided that we will take a given point and add or subtract 25 years. Since we cannot exactly go into the future from the present, the modern era is only 25 years and is from the mid-90s to today. We also researched two other key periods that had many landmark cases relating to the second amendment. We did not give them any specific name; they were from 1875 to 1925 and from 1940 to 1990. We wanted to make sure each time period not only contained its fair share of court cases for us to retrieve valuable information from, we also wanted to ensure that the key terms did not change their meanings much over these periods. While there was a large jump in military technology in our earliest era during the Great War, most of the court cases precede that, so we concluded it would not be an issue. By the start of our middle era, machine guns had already been invented and were mainstream (not in the home, but many knew what they were), and with the exclusion of nuclear armaments, stealth technology, napalm, and other types of general weaponry that is generally agreed upon that individual Americans do not intrinsically have a right to without proper training and certification, such as military personnel, there was not much development after this point. Thus, we decided that these were good time periods to discuss, and we gathered many court cases and their opinions and transcribed them into .txt files for the CS division to begin testing on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Development\n",
    "## Overview / Background for Math Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our team's goal with this project is to develop a process for measuring and visualizing the evolving sense of controversial constitutional terminology. The bulk of this work hinges on Word2Vec, an NLP API that allows for the embedding of words as vectors. In vectorizing words, we open up avenues for the analysis of linguistic relationships through geometric means. \n",
    "\n",
    "In essence the idea is, given a set of historical documents, we want to generate the \"optimal\" embedding (which we will discuss more later), and then try to extract a geometric context for controversial words, such as \"arms\". \n",
    "\n",
    "Word embeddings need to capture an incredible amount of complexity and variation, so the word vectors we produce have dimension on the order of tens. In these high-dimensional spaces, the Euclidean norm starts to lose it's ability to \"fairly\" represent distance, so we instead use the cosine-norm for all distance calculations.\n",
    "\n",
    "$$ cosine\\_norm(u,v) = \\frac{u \\cdot v}{|u|~|v|} $$ \n",
    "\n",
    "Another side-effect of high-dimensions is the obvious difficulty to produce an intelligible visualization. For this reason, in the Network Creation section of our project, we implemented Singular Value Decomposition. SVD is a process for decomposing a rank-r matrix into a sum of r rank-1 matrices. \n",
    "\n",
    "Suppose our word embedding process has generated a set of 2000 word vectors of dimension 40 (these are realistic values for this project). These vectors \"live\" in the standard basis of $R^{40}$. Let's see if we can simplify that basis into something more amenable to visualization while losing as little information as possible. We place all of our vectors into a matrix A as row vectors.\n",
    "$$ A = \\sigma_1 \\vec{u_1} \\vec{v_1}^T + \\sigma_2 \\vec{u_2} \\vec{v_2}^T ... + \\sigma_r \\vec{u_r} \\vec{v_r}^T$$\n",
    "It is relevant here because, much like diagonalization, it allows one to obtain a potentially more useful basis for a set of vectors. For our networks, we populate $A$ with our word vectors as rows, and then attain an approximation for A by summing our  three $\\sigma_i \\vec{u_i} \\vec{v_i}^T$ terms with highest corresponding singular values ($\\sigma_1,\\sigma_2,\\sigma_3$). Another way of saying this is (supposing we order our singular values in descending order):\n",
    "\n",
    "$$A \\approx \\sigma_1 \\vec{u_1} \\vec{v_1}^T + \\sigma_2 \\vec{u_2} \\vec{v_2}^T + \\sigma_3 \\vec{u_3} \\vec{v_3}^T$$\n",
    "\n",
    "This approximation allows us to create a new, 3-dimensional basis for our word vectors:\n",
    "\n",
    "$$B_{reduced} = \\{ \\vec{v_1},\\vec{v_2},\\vec{v_3}\\} $$\n",
    "\n",
    "The basis vectors for $B_{reduced}$ still live in $R^{40}$. However, by projecting each of our word vectors onto this basis, we can achieve an approximation for our word vectors in $R^3$. This lets us visualize the structure of a 40-dimensional space with a graph in 3 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  * For each era:\n",
    "    * Parse the relevant set of documents\n",
    "    * For each \"reasonable\" set of constructor parameters\n",
    "      * Initialize a Word2Vec model with parameters\n",
    "      * Calculate WordSim353 similarity Spearmen correlation for model\n",
    "    * Choose the model which maximizes WordSim353 Spearmen\n",
    "    * Analyze the \"neighborhood\" of chosen words through:\n",
    "      * Creation of a graph based on Word2Vec's word similarity metric\n",
    "      %* K-Means Clustering of embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "## Parsing\n",
    "\n",
    "In this step, we take in a set of text documents and tokenize it by sentences and then by words. The result is a list of lists of words, which we will pass to the Word2Vec constructor shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from word_embeddings_benchmarks_master.web.datasets.similarity import fetch_WS353\n",
    "from cluster import Clustering\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import sys\n",
    "import scipy\n",
    "from six import text_type\n",
    "from six import PY2\n",
    "from six import iteritems\n",
    "from six import string_types\n",
    "from word_embeddings_benchmarks_master.web.utils import _open\n",
    "from word_embeddings_benchmarks_master.web.vocabulary import *\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "from functools import partial\n",
    "from word_embeddings_benchmarks_master.web.utils import standardize_string, to_utf8\n",
    "from word_embeddings_benchmarks_master.web.embedding import Embedding\n",
    "from sklearn.metrics import pairwise_distances\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def evaluate_similarity(w, X, y):\n",
    "    \"\"\"\n",
    "    Calculate Spearman correlation between cosine similarity of the model\n",
    "    and human rated similarity of word pairs\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : Embedding or dict\n",
    "      Embedding or dict instance.\n",
    "\n",
    "    X: array, shape: (n_samples, 2)\n",
    "      Word pairs\n",
    "\n",
    "    y: vector, shape: (n_samples,)\n",
    "      Human ratings\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cor: float\n",
    "      Spearman correlation\n",
    "    \"\"\"\n",
    "    if isinstance(w, dict):\n",
    "        w = Embedding.from_dict(w)\n",
    "\n",
    "    missing_words = 0\n",
    "    words = w.vocabulary.word_id\n",
    "    for query in X:\n",
    "        for query_word in query:\n",
    "            if query_word not in words:\n",
    "                missing_words += 1\n",
    "    if missing_words > 0:\n",
    "        logger.warning(\"Missing {} words. Will replace them with mean vector\".format(missing_words))\n",
    "\n",
    "\n",
    "    mean_vector = np.mean(w.vectors, axis=0, keepdims=True)\n",
    "    A = np.vstack(w.get(word, mean_vector) for word in X[:, 0])\n",
    "    B = np.vstack(w.get(word, mean_vector) for word in X[:, 1])\n",
    "    scores = np.array([v1.dot(v2.T)/(np.linalg.norm(v1)*np.linalg.norm(v2)) for v1, v2 in zip(A, B)])\n",
    "    return scipy.stats.spearmanr(scores, y).correlation\n",
    "\n",
    "def eval_sim(model):\n",
    "    d = {word:model.wv[word] for word in model.wv.vocab}\n",
    "    data = fetch_WS353(which=\"similarity\")\n",
    "    return evaluate_similarity(d, data.X, data.y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec as Word2Vec\n",
    "import sys, re, os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import word_embeddings_benchmarks_master\n",
    "import tqdm\n",
    "from word_embeddings_benchmarks_master.web import * \n",
    "SENTENCE_TOKENIZER = nltk.data.load('./nltk_data/tokenizers/punkt/english.pickle') \n",
    "QUOTES = re.compile(\"\\u201c|\\u201d\")\n",
    "\n",
    "def read_dir(path):\n",
    "    \"\"\"\n",
    "    @param path: (type = str) path to dir that containes files\n",
    "    @return: (type=str)\n",
    "    \"\"\"\n",
    "    assert os.path.exists(path)\n",
    "    file_names = os.listdir(path)\n",
    "    return read(file_names, path)\n",
    "\n",
    "def read(file_names, path):\n",
    "    \"\"\"\n",
    "    Reads in a text file as a string, returning the stringified version\n",
    "    @param file_names: (type=list<str>) files to be read\n",
    "    @param path: (type = str) path to dir that containes files\n",
    "    @return: (type=str)\n",
    "    \"\"\"\n",
    "    data = \"\"\n",
    "    for file_name in file_names:\n",
    "        with open(path + \"/\" + file_name, \"r\", encoding = \"utf-8\") as f:\n",
    "            data += re.sub(QUOTES, \"\\\"\", f.read())\n",
    "            data += \"\\n\"\n",
    "    return data\n",
    "\n",
    "def parse(text):\n",
    "    \"\"\"\n",
    "    Parses a stringified file into sentences, a list of lists of words\n",
    "    @param text: (type=str) text to parse\n",
    "    @return: (type=list<list<str>>) parsed text \n",
    "    \"\"\"\n",
    "    sentences = SENTENCE_TOKENIZER.tokenize(text)\n",
    "    sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "data = read_dir(\"Modern_Era/2A\")\n",
    "sentences = parse(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3\n",
    "## Embedding/Optimization\n",
    "\n",
    "We now take the parsed text from the previous step, and pass it to our embedding initializer. We chose 3 model parameters to optimize over:\n",
    "* size: the dimension of the produced word vectors\n",
    "* window: the 'radius' around a word that Word2Vec will inspect\n",
    "* min_count: the minimum frequency for a word to be included in the model\n",
    "\n",
    "We chose acceptable ranges for each of those parameters and then evaluated every Word2Vec embedding for this particular text within those parameter ranges. We use WordSim353's similarity metric, which measures the correlation of cosine similarity of word vectors with human reported similarity of words. The model that we output is the model with the highest WordSim353 similarity correlation (as similarity is central to our processing of the embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing 298 words. Will replace them with mean vector\n",
      "Missing 328 words. Will replace them with mean vector\n",
      "Missing 339 words. Will replace them with mean vector\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b17a6c87bd1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;31m#model.save(\"second_amendment_modern_demo.bin\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"second_amendment.bin\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-b17a6c87bd1b>\u001b[0m in \u001b[0;36minit_model\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mwindow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m                 \u001b[0msimilarity_spearmen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_sim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msimilarity_spearmen\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_spearmen\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Duffy\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss)\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m             self.train(sentences, total_examples=self.corpus_count, epochs=self.iter,\n\u001b[1;32m--> 505\u001b[1;33m                        start_alpha=self.alpha, end_alpha=self.min_alpha)\n\u001b[0m\u001b[0;32m    506\u001b[0m         \u001b[1;32melse\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Duffy\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss)\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocks if workers too slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    953\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# a thread reporting that it finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Duffy\\Anaconda3\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Duffy\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def init_model(sentences):\n",
    "    \"\"\"\n",
    "    Initializes optimal Word2Vec model for text from sentences\n",
    "    @param sentences: (type=list<list<str>>) a list of lists of words\n",
    "    @return: (type=Word2Vec) model\n",
    "    \"\"\"\n",
    "    best_spearmen = 0\n",
    "    best_model = None\n",
    "    best_params = (0,0,0)\n",
    "    count = 0\n",
    "    for dim in range(20, 61):\n",
    "        for window in range(5, 11):\n",
    "            for min_count in range(2,5):\n",
    "                model = Word2Vec(sentences, min_count=min_count, window=window, size=dim)\n",
    "                similarity_spearmen = eval_sim(model)\n",
    "                if similarity_spearmen > best_spearmen:\n",
    "                    best_model = model\n",
    "                    best_params = (min_count, window, dim)\n",
    "                    best_spearmen = similarity_spearmen\n",
    "                #print(str(count) + \": \" + str(similarity_spearmen))\n",
    "                count += 1\n",
    "    print(\"min_count: \" + str(best_params[0]), \n",
    "        \"window: \" + str(best_params[1]), \n",
    "        \"dim: \" + str(best_params[2]))\n",
    "    print(\"correlation: \" + str(best_spearmen))\n",
    "    return best_model\n",
    "\n",
    "model = init_model(sentences)\n",
    "#model.save(\"second_amendment_modern_demo.bin\")\n",
    "model.save(\"second_amendment.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT:\n",
    "\n",
    "The code above does, in fact, properly function. However, since it outputs an enormous amount of text when it is run (because of a missing word command within our eval_sim method, I terminated the code prematurely so as to avoid taking up too much space. Feel free to run it (you will need to if you wish to go through everything or if the plots below are not loaded properly), however it is simply a repetition of the three lines above many, many times. Also, be aware it will take some time as it is processing through thousands of words in all of these documents numerous times in order to optimize the embedding. The output, in the end, is:\n",
    "\n",
    "min_count: 2 window: 9 dim: 32 <br />\n",
    "correlation: 0.331644710395"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4\n",
    "## Neighborhood Creation/Dimensionality Reduction\n",
    "\n",
    "We now use Word2Vec's built in similarity measure to extract the 10 most similar words to our \"main_word\". This set of 11 words constitutes a \"neighborhood\". We perform SVD on this neighborhood of word vectors and pass the reduced dimension word vectors along to the visualization stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Neighborhood:\n",
    "    def __init__(self, word, model, neighbors=10):\n",
    "        \"\"\"\n",
    "        @param word: (type=str) \n",
    "        @param model: (type=Word2Vec Model) <(neighboring_word, edge_weight)>\n",
    "        @field word: (type=str)\n",
    "        @field similarity_neighbors: (type=list<tuple<str, float>>) neighbors to word based on \n",
    "        @field proximity_neighbors: (type=list<tuple<str, float>>) neighbors to word based on cosine_distance \n",
    "        \"\"\"\n",
    "        self.word = word\n",
    "        self.similarity_neighbors = model.wv.most_similar(positive=[word], topn=neighbors)\n",
    "\n",
    "def get_neighboring_words(word, model, n=10, verbose=False):\n",
    "    n = Neighborhood(word, model, neighbors=n)\n",
    "    if verbose:\n",
    "        print(n.word)\n",
    "        print(\"Similarity Neighbors\")\n",
    "    words = []\n",
    "    for neighbor in n.similarity_neighbors:\n",
    "        if verbose:\n",
    "            print(neighbor)\n",
    "        words.append(neighbor[0])\n",
    "    return words, n.similarity_neighbors\n",
    "\n",
    "def get_svd_from_words(model, words, verbose=False):\n",
    "    vecs = [model.wv[word] for word in words]\n",
    "    mat = np.stack(vecs, axis=0)\n",
    "    if verbose:\n",
    "        print('shape of word embeddngs matrix:', mat.shape) #shape is (neighbors,32), neighbors defaults to 10\n",
    "    U, s, V = np.linalg.svd(mat)\n",
    "    if verbose:\n",
    "        print('singular values:', s)   # Take a quick look at svd_test.py (run it) if you want to convince yourself of how svd works for m by n matrices\n",
    "               # We basically want the first three row vectors in V; these are the eigenvectors that explain most of the variation in the rows (i.e. word embeddings) of the original matrix.\n",
    "    return U, s, V\n",
    "\n",
    "def get_coords_from_svd_projection(V, model, words, verbose=False):\n",
    "    V_cut = V[:3,:]\n",
    "    if verbose:\n",
    "        print('matrix after removing less import eigenvectors:')\n",
    "        print(V_cut)\n",
    "        print('squared row magnitudes:')\n",
    "        for i in range(3):     # this yields 1.0 every time, i.e. to compute projection coordinates we can ignore the a.a on the denominator (V_cut has unitary rows)\n",
    "            print(V_cut[i,:].dot(V_cut[i,:]))\n",
    "\n",
    "\n",
    "    #for each word, associate it with its projection coordinates in the V_cut basis.\n",
    "    # This associates each words with a ordered triple of points, which allows us to graph in 3d,\n",
    "    # or 2d (you can just use the first two coordinates if you want). One idea for the future would\n",
    "    #be to label the axes of the graph with the word that the corresponding basis vector of our graph is closest to.\n",
    "    return V_cut, {w: V_cut.dot(model.wv[w]) for w in words}\n",
    "\n",
    "\"\"\"\n",
    "Main utility method for any users of this file. Bundles up three sub-processes to allow you \n",
    "to get a three dimensional space on which to plot the similar words to the given one based on some model\n",
    "\n",
    "The current method computes the svd of the similar words, along with the original word, but throws out words of length\n",
    "2 or smaller by default.\n",
    "\"\"\"\n",
    "def get_points_from_word_and_model(word, model_path, verbose=False, bigger_than=2):\n",
    "    model = Word2Vec.load(model_path)\n",
    "    \n",
    "    #gets the n \"most similar\" words to the initial word in this model. \n",
    "    #Also returns a dict containing those similarity scores, which is not used in this method\n",
    "    words,_ = get_neighboring_words(word,model, n=10, verbose=verbose)\n",
    "\n",
    "    # an intermediate processing step here that removes small words and adds in the main word we're considering?\n",
    "    cond = (lambda w: len(w) > bigger_than) if bigger_than > 0 else (lambda w: True)\n",
    "    words = [word] + [w for w in words if cond(w)]\n",
    "\n",
    "    # computes the svd of the matrix containing the embeddings of the 10 most similar words as rows. \n",
    "    # U and s are not used, but are left in for readability. s contains the singular values, set verbose=True to print them.\n",
    "    U, s, V = get_svd_from_words(model, words, verbose=verbose)\n",
    "    \n",
    "    basis, coords = get_coords_from_svd_projection(V, model, words, verbose=verbose)\n",
    "\n",
    "    return basis, coords, model\n",
    "\n",
    "word1 = \"arms\"\n",
    "model_path = \"second_amendment.bin\"\n",
    "basis1, coords1, _1 = get_points_from_word_and_model(word1, model_path)\n",
    "word2 = \"militia\"\n",
    "basis2, coords2, _2 = get_points_from_word_and_model(word2, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5\n",
    "## Neighborhood Visualization\n",
    "In this stage, we plot the neighborhood in our new 3-dimensional basis and create a labeled visualization of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n",
      "<gensim.models.keyedvectors.KeyedVectors object at 0x000002E4E88DBD30>\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "hoverinfo": "none",
         "line": {
          "color": "rgb(125,125,125)",
          "width": 1
         },
         "mode": "lines",
         "type": "scatter3d",
         "x": [
          -10.056729316711426,
          -9.075739860534668,
          null,
          -10.056729316711426,
          -1.881561517715454,
          null,
          -10.056729316711426,
          -8.910184860229492,
          null,
          -10.056729316711426,
          -9.403715133666992,
          null,
          -10.056729316711426,
          -1.1379997730255127,
          null,
          -10.056729316711426,
          -6.029302597045898,
          null,
          -10.056729316711426,
          -0.583845853805542,
          null,
          -10.056729316711426,
          -1.8659610748291016,
          null,
          -10.056729316711426,
          -0.29499977827072144,
          null,
          -10.056729316711426,
          -0.5917984247207642,
          null
         ],
         "y": [
          -0.2418440282344818,
          -0.7438547015190125,
          null,
          -0.2418440282344818,
          -0.18534491956233978,
          null,
          -0.2418440282344818,
          0.8480539917945862,
          null,
          -0.2418440282344818,
          0.9245948195457458,
          null,
          -0.2418440282344818,
          0.025489237159490585,
          null,
          -0.2418440282344818,
          -0.9993277192115784,
          null,
          -0.2418440282344818,
          -0.10006795078516006,
          null,
          -0.2418440282344818,
          -0.3211297392845154,
          null,
          -0.2418440282344818,
          0.01791096106171608,
          null,
          -0.2418440282344818,
          -0.1189897283911705,
          null
         ],
         "z": [
          0.4738585352897644,
          -0.1537650227546692,
          null,
          0.4738585352897644,
          -0.06713321805000305,
          null,
          0.4738585352897644,
          -0.030815541744232178,
          null,
          0.4738585352897644,
          -0.18779024481773376,
          null,
          0.4738585352897644,
          -0.03942849859595299,
          null,
          0.4738585352897644,
          -0.26210230588912964,
          null,
          0.4738585352897644,
          0.004490397870540619,
          null,
          0.4738585352897644,
          0.2194928228855133,
          null,
          0.4738585352897644,
          -0.005998925305902958,
          null,
          0.4738585352897644,
          0.019643420353531837,
          null
         ]
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": "rgb(175,175,175)",
          "line": {
           "color": "rgb(50,50,50)",
           "width": 0.5
          },
          "size": 6,
          "symbol": "dot"
         },
         "mode": "markers",
         "text": [
          "arms",
          "right",
          "carry",
          "keep",
          "bear",
          "infringed",
          "people",
          "fire",
          "defence",
          "references",
          "suitable"
         ],
         "type": "scatter3d",
         "x": [
          -10.056729316711426,
          -9.075739860534668,
          -1.881561517715454,
          -8.910184860229492,
          -9.403715133666992,
          -1.1379997730255127,
          -6.029302597045898,
          -0.583845853805542,
          -1.8659610748291016,
          -0.29499977827072144,
          -0.5917984247207642
         ],
         "y": [
          -0.2418440282344818,
          -0.7438547015190125,
          -0.18534491956233978,
          0.8480539917945862,
          0.9245948195457458,
          0.025489237159490585,
          -0.9993277192115784,
          -0.10006795078516006,
          -0.3211297392845154,
          0.01791096106171608,
          -0.1189897283911705
         ],
         "z": [
          0.4738585352897644,
          -0.1537650227546692,
          -0.06713321805000305,
          -0.030815541744232178,
          -0.18779024481773376,
          -0.03942849859595299,
          -0.26210230588912964,
          0.004490397870540619,
          0.2194928228855133,
          -0.005998925305902958,
          0.019643420353531837
         ]
        }
       ],
       "layout": {
        "height": 1000,
        "hovermode": "closest",
        "margin": {
         "t": 100
        },
        "scene": {
         "xaxis": {
          "showbackground": false,
          "showgrid": false,
          "showline": false,
          "showticklabels": false,
          "title": " ",
          "zeroline": false
         },
         "yaxis": {
          "showbackground": false,
          "showgrid": false,
          "showline": false,
          "showticklabels": false,
          "title": " ",
          "zeroline": false
         },
         "zaxis": {
          "showbackground": false,
          "showgrid": false,
          "showline": false,
          "showticklabels": false,
          "title": " ",
          "zeroline": false
         }
        },
        "showlegend": false,
        "title": "word relationships with respect to arms",
        "width": 1000
       }
      },
      "text/html": [
       "<div id=\"86761170-34ed-48f4-9e13-d7e7b1419bf9\" style=\"height: 1000px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"86761170-34ed-48f4-9e13-d7e7b1419bf9\", [{\"type\": \"scatter3d\", \"x\": [-10.056729316711426, -9.075739860534668, null, -10.056729316711426, -1.881561517715454, null, -10.056729316711426, -8.910184860229492, null, -10.056729316711426, -9.403715133666992, null, -10.056729316711426, -1.1379997730255127, null, -10.056729316711426, -6.029302597045898, null, -10.056729316711426, -0.583845853805542, null, -10.056729316711426, -1.8659610748291016, null, -10.056729316711426, -0.29499977827072144, null, -10.056729316711426, -0.5917984247207642, null], \"y\": [-0.2418440282344818, -0.7438547015190125, null, -0.2418440282344818, -0.18534491956233978, null, -0.2418440282344818, 0.8480539917945862, null, -0.2418440282344818, 0.9245948195457458, null, -0.2418440282344818, 0.025489237159490585, null, -0.2418440282344818, -0.9993277192115784, null, -0.2418440282344818, -0.10006795078516006, null, -0.2418440282344818, -0.3211297392845154, null, -0.2418440282344818, 0.01791096106171608, null, -0.2418440282344818, -0.1189897283911705, null], \"z\": [0.4738585352897644, -0.1537650227546692, null, 0.4738585352897644, -0.06713321805000305, null, 0.4738585352897644, -0.030815541744232178, null, 0.4738585352897644, -0.18779024481773376, null, 0.4738585352897644, -0.03942849859595299, null, 0.4738585352897644, -0.26210230588912964, null, 0.4738585352897644, 0.004490397870540619, null, 0.4738585352897644, 0.2194928228855133, null, 0.4738585352897644, -0.005998925305902958, null, 0.4738585352897644, 0.019643420353531837, null], \"mode\": \"lines\", \"line\": {\"color\": \"rgb(125,125,125)\", \"width\": 1}, \"hoverinfo\": \"none\"}, {\"type\": \"scatter3d\", \"x\": [-10.056729316711426, -9.075739860534668, -1.881561517715454, -8.910184860229492, -9.403715133666992, -1.1379997730255127, -6.029302597045898, -0.583845853805542, -1.8659610748291016, -0.29499977827072144, -0.5917984247207642], \"y\": [-0.2418440282344818, -0.7438547015190125, -0.18534491956233978, 0.8480539917945862, 0.9245948195457458, 0.025489237159490585, -0.9993277192115784, -0.10006795078516006, -0.3211297392845154, 0.01791096106171608, -0.1189897283911705], \"z\": [0.4738585352897644, -0.1537650227546692, -0.06713321805000305, -0.030815541744232178, -0.18779024481773376, -0.03942849859595299, -0.26210230588912964, 0.004490397870540619, 0.2194928228855133, -0.005998925305902958, 0.019643420353531837], \"mode\": \"markers\", \"marker\": {\"symbol\": \"dot\", \"size\": 6, \"color\": \"rgb(175,175,175)\", \"line\": {\"color\": \"rgb(50,50,50)\", \"width\": 0.5}}, \"text\": [\"arms\", \"right\", \"carry\", \"keep\", \"bear\", \"infringed\", \"people\", \"fire\", \"defence\", \"references\", \"suitable\"], \"hoverinfo\": \"text\"}], {\"title\": \"word relationships with respect to arms\", \"width\": 1000, \"height\": 1000, \"showlegend\": false, \"scene\": {\"xaxis\": {\"showbackground\": false, \"showline\": false, \"zeroline\": false, \"showgrid\": false, \"showticklabels\": false, \"title\": \" \"}, \"yaxis\": {\"showbackground\": false, \"showline\": false, \"zeroline\": false, \"showgrid\": false, \"showticklabels\": false, \"title\": \" \"}, \"zaxis\": {\"showbackground\": false, \"showline\": false, \"zeroline\": false, \"showgrid\": false, \"showticklabels\": false, \"title\": \" \"}}, \"margin\": {\"t\": 100}, \"hovermode\": \"closest\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"86761170-34ed-48f4-9e13-d7e7b1419bf9\" style=\"height: 1000px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"86761170-34ed-48f4-9e13-d7e7b1419bf9\", [{\"type\": \"scatter3d\", \"x\": [-10.056729316711426, -9.075739860534668, null, -10.056729316711426, -1.881561517715454, null, -10.056729316711426, -8.910184860229492, null, -10.056729316711426, -9.403715133666992, null, -10.056729316711426, -1.1379997730255127, null, -10.056729316711426, -6.029302597045898, null, -10.056729316711426, -0.583845853805542, null, -10.056729316711426, -1.8659610748291016, null, -10.056729316711426, -0.29499977827072144, null, -10.056729316711426, -0.5917984247207642, null], \"y\": [-0.2418440282344818, -0.7438547015190125, null, -0.2418440282344818, -0.18534491956233978, null, -0.2418440282344818, 0.8480539917945862, null, -0.2418440282344818, 0.9245948195457458, null, -0.2418440282344818, 0.025489237159490585, null, -0.2418440282344818, -0.9993277192115784, null, -0.2418440282344818, -0.10006795078516006, null, -0.2418440282344818, -0.3211297392845154, null, -0.2418440282344818, 0.01791096106171608, null, -0.2418440282344818, -0.1189897283911705, null], \"z\": [0.4738585352897644, -0.1537650227546692, null, 0.4738585352897644, -0.06713321805000305, null, 0.4738585352897644, -0.030815541744232178, null, 0.4738585352897644, -0.18779024481773376, null, 0.4738585352897644, -0.03942849859595299, null, 0.4738585352897644, -0.26210230588912964, null, 0.4738585352897644, 0.004490397870540619, null, 0.4738585352897644, 0.2194928228855133, null, 0.4738585352897644, -0.005998925305902958, null, 0.4738585352897644, 0.019643420353531837, null], \"mode\": \"lines\", \"line\": {\"color\": \"rgb(125,125,125)\", \"width\": 1}, \"hoverinfo\": \"none\"}, {\"type\": \"scatter3d\", \"x\": [-10.056729316711426, -9.075739860534668, -1.881561517715454, -8.910184860229492, -9.403715133666992, -1.1379997730255127, -6.029302597045898, -0.583845853805542, -1.8659610748291016, -0.29499977827072144, -0.5917984247207642], \"y\": [-0.2418440282344818, -0.7438547015190125, -0.18534491956233978, 0.8480539917945862, 0.9245948195457458, 0.025489237159490585, -0.9993277192115784, -0.10006795078516006, -0.3211297392845154, 0.01791096106171608, -0.1189897283911705], \"z\": [0.4738585352897644, -0.1537650227546692, -0.06713321805000305, -0.030815541744232178, -0.18779024481773376, -0.03942849859595299, -0.26210230588912964, 0.004490397870540619, 0.2194928228855133, -0.005998925305902958, 0.019643420353531837], \"mode\": \"markers\", \"marker\": {\"symbol\": \"dot\", \"size\": 6, \"color\": \"rgb(175,175,175)\", \"line\": {\"color\": \"rgb(50,50,50)\", \"width\": 0.5}}, \"text\": [\"arms\", \"right\", \"carry\", \"keep\", \"bear\", \"infringed\", \"people\", \"fire\", \"defence\", \"references\", \"suitable\"], \"hoverinfo\": \"text\"}], {\"title\": \"word relationships with respect to arms\", \"width\": 1000, \"height\": 1000, \"showlegend\": false, \"scene\": {\"xaxis\": {\"showbackground\": false, \"showline\": false, \"zeroline\": false, \"showgrid\": false, \"showticklabels\": false, \"title\": \" \"}, \"yaxis\": {\"showbackground\": false, \"showline\": false, \"zeroline\": false, \"showgrid\": false, \"showticklabels\": false, \"title\": \" \"}, \"zaxis\": {\"showbackground\": false, \"showline\": false, \"zeroline\": false, \"showgrid\": false, \"showticklabels\": false, \"title\": \" \"}}, \"margin\": {\"t\": 100}, \"hovermode\": \"closest\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "hoverinfo": "none",
         "line": {
          "color": "rgb(125,125,125)",
          "width": 1
         },
         "mode": "lines",
         "type": "scatter3d",
         "x": [
          -7.0010576248168945,
          -6.802945137023926,
          null,
          -7.0010576248168945,
          -3.230569839477539,
          null,
          -7.0010576248168945,
          -1.0546475648880005,
          null,
          -7.0010576248168945,
          -2.8160223960876465,
          null,
          -7.0010576248168945,
          -6.455268859863281,
          null,
          -7.0010576248168945,
          -6.203476905822754,
          null,
          -7.0010576248168945,
          -1.7640821933746338,
          null,
          -7.0010576248168945,
          -1.1998176574707031,
          null
         ],
         "y": [
          0.16536013782024384,
          0.2445136308670044,
          null,
          0.16536013782024384,
          -0.06240164488554001,
          null,
          0.16536013782024384,
          0.007937340065836906,
          null,
          0.16536013782024384,
          -0.006016485393047333,
          null,
          0.16536013782024384,
          -0.21418896317481995,
          null,
          0.16536013782024384,
          -0.23649996519088745,
          null,
          0.16536013782024384,
          0.1455310881137848,
          null,
          0.16536013782024384,
          -0.014921605587005615,
          null
         ],
         "z": [
          -0.16222476959228516,
          0.175365149974823,
          null,
          -0.16222476959228516,
          -0.11259345710277557,
          null,
          -0.16222476959228516,
          -0.016391098499298096,
          null,
          -0.16222476959228516,
          -0.05728384852409363,
          null,
          -0.16222476959228516,
          0.03149464726448059,
          null,
          -0.16222476959228516,
          0.04795989394187927,
          null,
          -0.16222476959228516,
          -0.032254211604595184,
          null,
          -0.16222476959228516,
          0.034306637942790985,
          null
         ]
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": "rgb(175,175,175)",
          "line": {
           "color": "rgb(50,50,50)",
           "width": 0.5
          },
          "size": 6,
          "symbol": "dot"
         },
         "mode": "markers",
         "text": [
          "militia",
          "state",
          "used",
          "secure",
          "purpose",
          "only",
          "they",
          "citizen",
          "merely"
         ],
         "type": "scatter3d",
         "x": [
          -7.0010576248168945,
          -6.802945137023926,
          -3.230569839477539,
          -1.0546475648880005,
          -2.8160223960876465,
          -6.455268859863281,
          -6.203476905822754,
          -1.7640821933746338,
          -1.1998176574707031
         ],
         "y": [
          0.16536013782024384,
          0.2445136308670044,
          -0.06240164488554001,
          0.007937340065836906,
          -0.006016485393047333,
          -0.21418896317481995,
          -0.23649996519088745,
          0.1455310881137848,
          -0.014921605587005615
         ],
         "z": [
          -0.16222476959228516,
          0.175365149974823,
          -0.11259345710277557,
          -0.016391098499298096,
          -0.05728384852409363,
          0.03149464726448059,
          0.04795989394187927,
          -0.032254211604595184,
          0.034306637942790985
         ]
        }
       ],
       "layout": {
        "height": 1000,
        "hovermode": "closest",
        "margin": {
         "t": 100
        },
        "scene": {
         "xaxis": {
          "showbackground": false,
          "showgrid": false,
          "showline": false,
          "showticklabels": false,
          "title": " ",
          "zeroline": false
         },
         "yaxis": {
          "showbackground": false,
          "showgrid": false,
          "showline": false,
          "showticklabels": false,
          "title": " ",
          "zeroline": false
         },
         "zaxis": {
          "showbackground": false,
          "showgrid": false,
          "showline": false,
          "showticklabels": false,
          "title": " ",
          "zeroline": false
         }
        },
        "showlegend": false,
        "title": "word relationships with respect to militia",
        "width": 1000
       }
      },
      "text/html": [
       "<div id=\"7f08a423-1ff8-4868-a0cb-ffbfb458c30a\" style=\"height: 1000px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"7f08a423-1ff8-4868-a0cb-ffbfb458c30a\", [{\"type\": \"scatter3d\", \"x\": [-7.0010576248168945, -6.802945137023926, null, -7.0010576248168945, -3.230569839477539, null, -7.0010576248168945, -1.0546475648880005, null, -7.0010576248168945, -2.8160223960876465, null, -7.0010576248168945, -6.455268859863281, null, -7.0010576248168945, -6.203476905822754, null, -7.0010576248168945, -1.7640821933746338, null, -7.0010576248168945, -1.1998176574707031, null], \"y\": [0.16536013782024384, 0.2445136308670044, null, 0.16536013782024384, -0.06240164488554001, null, 0.16536013782024384, 0.007937340065836906, null, 0.16536013782024384, -0.006016485393047333, null, 0.16536013782024384, -0.21418896317481995, null, 0.16536013782024384, -0.23649996519088745, null, 0.16536013782024384, 0.1455310881137848, null, 0.16536013782024384, -0.014921605587005615, null], \"z\": [-0.16222476959228516, 0.175365149974823, null, -0.16222476959228516, -0.11259345710277557, null, -0.16222476959228516, -0.016391098499298096, null, -0.16222476959228516, -0.05728384852409363, null, -0.16222476959228516, 0.03149464726448059, null, -0.16222476959228516, 0.04795989394187927, null, -0.16222476959228516, -0.032254211604595184, null, -0.16222476959228516, 0.034306637942790985, null], \"mode\": \"lines\", \"line\": {\"color\": \"rgb(125,125,125)\", \"width\": 1}, \"hoverinfo\": \"none\"}, {\"type\": \"scatter3d\", \"x\": [-7.0010576248168945, -6.802945137023926, -3.230569839477539, -1.0546475648880005, -2.8160223960876465, -6.455268859863281, -6.203476905822754, -1.7640821933746338, -1.1998176574707031], \"y\": [0.16536013782024384, 0.2445136308670044, -0.06240164488554001, 0.007937340065836906, -0.006016485393047333, -0.21418896317481995, -0.23649996519088745, 0.1455310881137848, -0.014921605587005615], \"z\": [-0.16222476959228516, 0.175365149974823, -0.11259345710277557, -0.016391098499298096, -0.05728384852409363, 0.03149464726448059, 0.04795989394187927, -0.032254211604595184, 0.034306637942790985], \"mode\": \"markers\", \"marker\": {\"symbol\": \"dot\", \"size\": 6, \"color\": \"rgb(175,175,175)\", \"line\": {\"color\": \"rgb(50,50,50)\", \"width\": 0.5}}, \"text\": [\"militia\", \"state\", \"used\", \"secure\", \"purpose\", \"only\", \"they\", \"citizen\", \"merely\"], \"hoverinfo\": \"text\"}], {\"title\": \"word relationships with respect to militia\", \"width\": 1000, \"height\": 1000, \"showlegend\": false, \"scene\": {\"xaxis\": {\"showbackground\": false, \"showline\": false, \"zeroline\": false, \"showgrid\": false, \"showticklabels\": false, \"title\": \" \"}, \"yaxis\": {\"showbackground\": false, \"showline\": false, \"zeroline\": false, \"showgrid\": false, \"showticklabels\": false, \"title\": \" \"}, \"zaxis\": {\"showbackground\": false, \"showline\": false, \"zeroline\": false, \"showgrid\": false, \"showticklabels\": false, \"title\": \" \"}}, \"margin\": {\"t\": 100}, \"hovermode\": \"closest\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"7f08a423-1ff8-4868-a0cb-ffbfb458c30a\" style=\"height: 1000px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"7f08a423-1ff8-4868-a0cb-ffbfb458c30a\", [{\"type\": \"scatter3d\", \"x\": [-7.0010576248168945, -6.802945137023926, null, -7.0010576248168945, -3.230569839477539, null, -7.0010576248168945, -1.0546475648880005, null, -7.0010576248168945, -2.8160223960876465, null, -7.0010576248168945, -6.455268859863281, null, -7.0010576248168945, -6.203476905822754, null, -7.0010576248168945, -1.7640821933746338, null, -7.0010576248168945, -1.1998176574707031, null], \"y\": [0.16536013782024384, 0.2445136308670044, null, 0.16536013782024384, -0.06240164488554001, null, 0.16536013782024384, 0.007937340065836906, null, 0.16536013782024384, -0.006016485393047333, null, 0.16536013782024384, -0.21418896317481995, null, 0.16536013782024384, -0.23649996519088745, null, 0.16536013782024384, 0.1455310881137848, null, 0.16536013782024384, -0.014921605587005615, null], \"z\": [-0.16222476959228516, 0.175365149974823, null, -0.16222476959228516, -0.11259345710277557, null, -0.16222476959228516, -0.016391098499298096, null, -0.16222476959228516, -0.05728384852409363, null, -0.16222476959228516, 0.03149464726448059, null, -0.16222476959228516, 0.04795989394187927, null, -0.16222476959228516, -0.032254211604595184, null, -0.16222476959228516, 0.034306637942790985, null], \"mode\": \"lines\", \"line\": {\"color\": \"rgb(125,125,125)\", \"width\": 1}, \"hoverinfo\": \"none\"}, {\"type\": \"scatter3d\", \"x\": [-7.0010576248168945, -6.802945137023926, -3.230569839477539, -1.0546475648880005, -2.8160223960876465, -6.455268859863281, -6.203476905822754, -1.7640821933746338, -1.1998176574707031], \"y\": [0.16536013782024384, 0.2445136308670044, -0.06240164488554001, 0.007937340065836906, -0.006016485393047333, -0.21418896317481995, -0.23649996519088745, 0.1455310881137848, -0.014921605587005615], \"z\": [-0.16222476959228516, 0.175365149974823, -0.11259345710277557, -0.016391098499298096, -0.05728384852409363, 0.03149464726448059, 0.04795989394187927, -0.032254211604595184, 0.034306637942790985], \"mode\": \"markers\", \"marker\": {\"symbol\": \"dot\", \"size\": 6, \"color\": \"rgb(175,175,175)\", \"line\": {\"color\": \"rgb(50,50,50)\", \"width\": 0.5}}, \"text\": [\"militia\", \"state\", \"used\", \"secure\", \"purpose\", \"only\", \"they\", \"citizen\", \"merely\"], \"hoverinfo\": \"text\"}], {\"title\": \"word relationships with respect to militia\", \"width\": 1000, \"height\": 1000, \"showlegend\": false, \"scene\": {\"xaxis\": {\"showbackground\": false, \"showline\": false, \"zeroline\": false, \"showgrid\": false, \"showticklabels\": false, \"title\": \" \"}, \"yaxis\": {\"showbackground\": false, \"showline\": false, \"zeroline\": false, \"showgrid\": false, \"showticklabels\": false, \"title\": \" \"}, \"zaxis\": {\"showbackground\": false, \"showline\": false, \"zeroline\": false, \"showgrid\": false, \"showticklabels\": false, \"title\": \" \"}}, \"margin\": {\"t\": 100}, \"hovermode\": \"closest\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "from nltk.cluster.util import cosine_distance\n",
    "#working example\n",
    "\"\"\"\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "\n",
    "#when using jupyter notebooks, uncomment the following line and change function call below to plotly.offline.iplot\n",
    "#plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "plotly.offline.plot({\n",
    "    \"data\": [Scatter(x=[1, 2, 3, 4], y=[4, 3, 2, 1])],\n",
    "    \"layout\": Layout(title=\"hello world\")\n",
    "})\n",
    "\"\"\"\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "print(basis1.shape[1])\n",
    "print(basis2.shape[1])\n",
    "print(model.wv)\n",
    "#coords should be a dictionary from a word to a 3d position\n",
    "def generate_network(main_word, coords):\n",
    "\n",
    "    #next step: generate edges.\n",
    "    #   In general, we can modify or optionize this however we want to get whatever edge structures we find interesting. \n",
    "    #   --We might, for example, create more Neighborhoods (one for each \"similar\" word) to see if we get any connections within the 1-level-away words.\n",
    "    #   --We could also try projecting some words from these new neighborhoods (maybe the top three from each?) onto our basis (just dot product) and getting those edges as well.\n",
    "    e_dict = {}\n",
    "    for i in range(3):\n",
    "        e_dict[coord_map[i] + '_e'] = sum([[main_coords[i], coords[w][i], None] for w in id_to_name if w != main_word], [])\n",
    "        e_dict[coord_map[i] + '_n'] = [coords[w][i] for w in id_to_name]\n",
    "\n",
    "    return e_dict\n",
    "\n",
    "def plot_network(g_dict, axis_titles):\n",
    "    trace1=Scatter3d(\n",
    "        x=g_dict['x_e'], y=g_dict['y_e'], z=g_dict['z_e'], mode='lines', line=Line(\n",
    "                    color='rgb(125,125,125)', width=1), hoverinfo='none')\n",
    "\n",
    "    trace2=Scatter3d(x=g_dict['x_n'], y=g_dict['y_n'], z=g_dict['z_n'], mode='markers', marker=Marker(\n",
    "        symbol='dot', size=6, color='rgb(175,175,175)', line = Line(\n",
    "            color='rgb(50,50,50)', width=0.5)), text=id_to_name, hoverinfo='text')\n",
    "\n",
    "    #print(axis_titles)\n",
    "    axes = [dict(showbackground=False, showline=False, zeroline=False, showgrid=False, showticklabels=False, title=' ') for i in range(3)]\n",
    " \n",
    "    title = 'word relationships with respect to ' + main_word\n",
    "    layout=Layout(title=title,width=1000, height=1000, showlegend=False, scene=Scene(\n",
    "        xaxis=XAxis(axes[0]), yaxis=YAxis(axes[1]), zaxis=ZAxis(axes[2])), margin=Margin(t=100), hovermode='closest')\n",
    "    \n",
    "    data = Data([trace1, trace2])\n",
    "    fig=Figure(data=data, layout=layout)\n",
    "    plotly.offline.iplot(fig, filename='arms_network.html')\n",
    "    \n",
    "def get_closest_basis_words(basis, model):\n",
    "    maps = {}\n",
    "    for i in range(basis.shape[0]):\n",
    "        curr_bas = basis[i,:]\n",
    "        closest = ('', 10)\n",
    "        for key in model.wv.vocab:\n",
    "#            if len(key) < 3:\n",
    "#                continue\n",
    "            word = key\n",
    "            vec = model.wv[key]\n",
    "            dist = abs(cosine_distance(vec, curr_bas))\n",
    "            if dist < closest[1]:\n",
    "                closest = (word, dist)\n",
    "        maps[i] = closest\n",
    "    return maps\n",
    "                \n",
    "\n",
    "\n",
    "#basis isn't going to be used yet, but we might label the axes in the future\n",
    "#generate node-id mappings\n",
    "main_word = word1\n",
    "name_to_id = {}\n",
    "id_to_name = []\n",
    "temp_id = 0\n",
    "for w in coords1:\n",
    "    name_to_id[w] = temp_id\n",
    "    temp_id +=1\n",
    "    id_to_name.append(w)\n",
    "main_coords = coords1[main_word] \n",
    "coord_map= ['x','y','z']\n",
    "\n",
    "#generate network\n",
    "g_dict1 = generate_network(word1, coords1)\n",
    "\n",
    "#plot network\n",
    "plot_network(g_dict1, get_closest_basis_words(basis, model))\n",
    "\n",
    "#basis isn't going to be used yet, but we might label the axes in the future\n",
    "#generate node-id mappings\n",
    "main_word = word2\n",
    "name_to_id = {}\n",
    "id_to_name = []\n",
    "temp_id = 0\n",
    "for w in coords2:\n",
    "    name_to_id[w] = temp_id\n",
    "    temp_id +=1\n",
    "    id_to_name.append(w)\n",
    "main_coords = coords2[main_word] \n",
    "coord_map= ['x','y','z']\n",
    "g_dict2 = generate_network(word2, coords2)\n",
    "\n",
    "#plot network\n",
    "plot_network(g_dict2, get_closest_basis_words(basis, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 6\n",
    "## Plot Analysis\n",
    "\n",
    "Above, we can see the finished product of the program. It maps, in a 3-D XYZ plane the relationship that our selected word, \"arms\" in our first case, has with the other words in all of the files in the selected section, the Modern Era. You can see each word by simply hovering over it and the plot is easy to manipulate and rotate for further analysis. Since this may not be as simply if you are simply looking at this as an image, we will list the words below. The point at the top center is arms. Otherwise, from left to right, top to bottom (then back to the top again):\n",
    "\n",
    "1. Defence\n",
    "2. Suitable\n",
    "3. Fire\n",
    "4. References\n",
    "5. Infringed\n",
    "6. Carry\n",
    "7. People\n",
    "8. Right\n",
    "9. Keep (Closer of the two on the far right)\n",
    "10. Bear (Farther of the two on the far right)\n",
    "\n",
    "One of the things that is clear is that we may need to remove the words that are around it within the second amendment from contention, so as to not pollute the word vector. Clearly, the word \"arms\" has no relation to the word \"infringed\" and a tenuous one to \"bear\", yet since they are in the 2nd amendment and are close by, they are listed. However, words like \"defence,\" \"fire\" as in firearms, and \"right\" are all valuable here. \"Right\" more so in the sense that right has a higher correlation than a word like \"privilege\", whereas the other two provide insight as to what the word arms means today. We can infer it has a high relation to \"defence\" of the individual as well as their family and their property, such as their home. \n",
    "\n",
    "For the second case, our word was \"milita\". Militia is the bottom right hand point. Otherwise, in a semicircular motion, going left and up then right and up then right and down:\n",
    "\n",
    "1. Citizen\n",
    "2. Secure\n",
    "3. Used (Going inward)\n",
    "4. Purpose\n",
    "5. Merely (Back outward)\n",
    "6. They\n",
    "7. Only\n",
    "8. State\n",
    "\n",
    "This visual only produced 8, not 10, since militia is not discussed nearly as much as arms in these cases as the judges do not believe it to be as central to the issue, and in many cases, the \"pro-gun\" side focuses on an individual's right to a firearm, not the militia. This also shows that our program needs some refinement. Again, words like \"state\"  appear near it in the text of the amendment and should probably be ignored. Of course, the extra words, like \"they\" and \"merely\" are for the most part inapplicable, so that does raise an issue. On the other hand, the word \"citizen\" is very important because it could reveal the \"pro-gun\" advocates are correct in saying that these militias are not necessarily state organized but rather can be composed entirely of the citizen body. While state is certainly present, it is much further away from militia in the plot, signaling a much weaker relationship. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7\n",
    "## Planning for the Future (Conclusion)\n",
    "\n",
    "From here, we know that our work is far from over, but we also know that we have come an incredible distance. From here, the humanities division will continue to research these eras and find more legal documents. We will expand from court cases into scholarly articles seeking to interpret the Constitution's meaning as well as amicus briefs related to the major court cases we have already looked into, as these provide even more insight to the issues. Additionally, I hope we can expand and begin to look into the fourth amendment in the future and perhaps several other debated parts of the Constitution. As for the CS division, with the visualization task being done (unless we decide to utilize the basis aspect again), the most important task ahead is refining the program to remove the words from the word vectors that, by and large, have almost no real connection to our selected words; in the past, we removed generic words and articles, and I think that should be expanded to include other generic words like \"they\" as well as most of the words used in the second amendment itself, excluding some particularly important ones, like arms and security. Not only are these important, they may certainly come up in other contexts surrounding a given word, whereas militia may not.\n",
    "\n",
    "While we unfortunately were unable to finish the project in its entirety by this semester's end, I am confident it can be concluded rapidly and early on into the Spring 2018 semester. Thank you!\n",
    "\n",
    "\n",
    "One last note:\n",
    "\n",
    "<br />I, Aidan Duffy, Textualism's Project Manager, would like to personally thank my team members for their committment and drive when it came to this project, especially after many of our initial team members decided to leave for a plethora of reasons. Specifically, I would like to thank Alex Cegarra for completing almost all of the foundational work for our algorithms by himself, as well as bringing Evan Lohn, his roommate and fellow CS division team member, to your team, as he helped out tremendously with refining our program as well as setting up our visualization. It has been an honor being this team's Project Manager, and I would hope I could continue in this position and you all continue on with PCS next semester and we can finish this together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "gensim API: https://radimrehurek.com/gensim/<br />\n",
    "Natural Language Toolkit: http://www.nltk.org/<br />\n",
    "Paper/Article on how Legal Robot implements word vectors: https://www.legalrobot.com/blog/2016/09/14/Word-Vectors/<br />\n",
    "Paper on predicitng law making using word vectors: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0176999 <br />\n",
    "\n",
    "Primary Source for Supreme Court opinions: https://www.justia.com/<br />\n",
    "Professor David Bamman, both at his office hours and his lectures(INFO 159): http://people.ischool.berkeley.edu/~dbamman/ <br />\n",
    "Profess Marti Hearst's research and presentations on NLP: http://people.ischool.berkeley.edu/~hearst/<br />\n",
    "Sinnot-Armstrong Paper on \"Word Meaning in Legal Intrepretation\": https://www.dropbox.com/s/yvn82seamomrsbr/Sinnott-Armstrong.pdf?dl=0<br />\n",
    "Stanford's (much, much worse :)) NLP that also uses gensim: https://nlp.stanford.edu/projects/histwords/<br />\n",
    "\n",
    "### List of Cases & Links\n",
    "**List of SCOTUS Cases:**<br />\n",
    "*Caetano v. Massachusetts, 2016, 1 Opinion: https://supreme.justia.com/cases/federal/us/577/14-10078/<br />\n",
    "District of Columbia v. Heller, 2008, Majority and 1 Dissenting: https://supreme.justia.com/cases/federal/us/554/570/<br />\n",
    "Lewis v. United States, 1966, Majority, 1 Concurring, 1 Dissent, Syllabus, and Footnotes*****:* https://supreme.justia.com/cases/federal/us/385/206/case.html<br />\n",
    "McDonald v. Chicago, 2010, Majority. 2 Concurring, and 2 Dissenting: https://supreme.justia.com/cases/federal/us/561/742/<br />\n",
    "Miller v. Texas, 1894, Majority, and the Syllabus: https://supreme.justia.com/cases/federal/us/153/535/case.html<br />\n",
    "Presser v. Illinois, 1886, Majority, and the Syllabus: https://supreme.justia.com/cases/federal/us/116/252/case.html<br />\n",
    "Robertson v. Baldwin, 1897, Majority, 1 Dissent, and the Syllabus: https://supreme.justia.com/cases/federal/us/165/275/case.html<br />\n",
    "United States v. Cruikshank, 1875, Majority, 1 Dissent, and the Syllabus:\n",
    "https://supreme.justia.com/cases/federal/us/92/542/case.html<br />\n",
    "United States v. Miller, 1939, Majority, Syllabus, and Footnotes*****:* https://supreme.justia.com/cases/federal/us/307/174/case.html<br />*\n",
    "\n",
    "**Note:** The rest only have the one majority opinion. <br />\n",
    "**Circuit Court Cases:**<br />\n",
    "*United States v. Emerson, 2001: https://aclu.procon.org/sourcefiles/US-v-Emerson.pdf<br />*\n",
    "**State Supreme Court Cases:**<br />\n",
    "*City of Salina v. Blasksley, 1905: http://www.guncite.com/court/state/83p619.html <br />\n",
    "People v. Aguilar, 2013, Overturning smaller court: http://www.illinoiscourts.gov/Opinions/SupremeCourt/2013/112116.pdf<br />*\n",
    "**State Court Cases:**<br />\n",
    "*People v. Aguilar, 2011: http://caselaw.findlaw.com/il-court-of-appeals/1557712.html <br />*\n",
    "<br />\n",
    "****IMPORTANT:** Those cases with a \"footnotes\" section had such an extensive notes section that including them in the original document would make it much too long. They warranted their own document since there were so many notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
